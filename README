lang: python3
modules: regex, collections, unicodedata, time

References: 
1:	http://norvig.com/spell-correct.html
2:	https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines
3:	http://ota.ahds.ac.uk/texts/0643.html

Terminology:
1:	ngram
2:	Memoization
3:	language model, error model

Requirements:
1:	basic spell checker(english, devnagri..): using bigram model.
2:	add trigram model
3:	form sentences from tags
4:	learning from script examples to correct bash commands

COMMENTS:
0:	Scripts (basic, enzyme, test1, test2, filecorct) are all independent examples.
	Will get the (test* & basic) into one.
	basic -- runs on small example set(<10) for testing if all works fastly :) 
	test1 -- medium set(270) test cases.
	test2 -- large set(400) test cases.
	enzyme -- to compare learned() with spelltest(), which set needs to be mentioned.
	filecorct â€” linked with myBigErrorsList in corpus.eng and tries to correct it.

	-------------------------------------
	[basic, test1, test2, filecorct] from python shell:
		>>> import basic
		>>> basic.main()
	-------------------------------------
	Corrections from python shell:
		>>> 
		>>> from tools.trainer import *
		>>> from tools.ngram import *
		>>> 
		>>> language = "english"
		>>> dword = "rint"
		>>>  _lang, NWORDS = setGlobalsWithLanguage(language);
		>>>  correct(dword);
		'ring'
	-------------------------------------

	For adding a new lang get a book inside corpus and make sure _inPath, _book, _lang parameters in tools.ngram.py is getting set under setGlobalsWithLanguage() and provide literals for lang inside tools.wordProcessor.charecterSet() which finally creates NGRAMS for the language.
	Best to add small example set under testCases.basic.test(), before running test1 or test2, will be faster to remove bugs :)  

1:	Bigram model(edit_distance 2) at max yet.
2:	The main difference between spelltest() and learned() methods is that learned() reduces differcence between correct(word) and target which helps in next runs(only if target is part of cndidates ie; reachable,  but P(target) < P(correct(word)) ). Hence next time the target has more chances of being chosen inside max(candidates).
	These points are provided from start with (bias > 0) runs, hence result differences bw learned() and spelltest() are not very high. 
	learned() adds error-model as EXPEXTENCE def.dict, from which bad cases can be reduced as we already know which word is expected helping during multiple runs. 

Runs with (bias=None) cases:
1:	Unknowns are 0 after 1st run only when caching in or inserting explicitly.
2:	After 1st, 2nd runs the difference in result is Null or random as the cases that remain can be only be improved further with edit_distance 3 algorithm included for the examples provided.



